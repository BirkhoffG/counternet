{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp training_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 31\n"
     ]
    }
   ],
   "source": [
    "# export\n",
    "from counternet.import_essentials import *\n",
    "from counternet.utils.all import *\n",
    "from counternet.evaluation import SensitivityMetric, ProximityMetric\n",
    "\n",
    "\n",
    "pl_logger = logging.getLogger('lightning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pl version: 1.3.5\ntorch version: 1.8.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"pl version: {pl.__version__}\")\n",
    "print(f\"torch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 774 ms, sys: 29.5 ms, total: 803 ms\nWall time: 802 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dummy_data = pd.read_csv('assets/data/dummy_data.csv')\n",
    "adult_data = load_adult_income_dataset('assets/data/adult.data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the categorical elements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export utils.processing\n",
    "class CategoricalNormalizer(object):\n",
    "    \"\"\"implement post-processing step to enforce each elements \n",
    "    in every category in the range of [0, 1] and output to 1.\n",
    "    \"\"\"\n",
    "    def __init__(self, categories: List[List[Any]], cat_idx: int):\n",
    "        self.categories = categories\n",
    "        self.cat_idx = cat_idx\n",
    "\n",
    "    def normalize(self, x, hard=False):\n",
    "        cat_idx = self.cat_idx\n",
    "        for col in self.categories:\n",
    "            cat_end_idx = cat_idx + len(col)\n",
    "            if hard:\n",
    "                x[:, cat_idx: cat_end_idx] = F.gumbel_softmax(x[:, cat_idx: cat_end_idx].clone().detach(), hard=hard)\n",
    "            else:\n",
    "                x[:, cat_idx: cat_end_idx] = F.softmax(x[:, cat_idx: cat_end_idx].clone().detach(), dim=-1)\n",
    "            cat_idx = cat_end_idx\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export evaluation\n",
    "class SensitivityMetric(Metric):\n",
    "    def __init__(self, predict_fn: Callable, scaler: ABCScaler, cat_idx: int, threshold: float):\n",
    "        super().__init__(dist_sync_on_step=False)\n",
    "        self.predict_fn = predict_fn\n",
    "        self.scaler = scaler\n",
    "        self.cat_idx = cat_idx\n",
    "        self.threshold = threshold\n",
    "\n",
    "        self.add_state(\"total_n_changes\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"diffs\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, x: torch.Tensor, c: torch.Tensor, c_y: torch.Tensor):\n",
    "        # inverse transform\n",
    "        x_cont_inv = self.scaler.inverse_transform(x[:, :self.cat_idx])\n",
    "        c_cont_inv = self.scaler.inverse_transform(c[:, :self.cat_idx])\n",
    "        # a bool metrics on whether differences between x and c is smaller than the threshold\n",
    "        cont_diff = torch.abs(x_cont_inv - c_cont_inv) < self.threshold\n",
    "        # total nums of differences\n",
    "        self.total_n_changes += torch.sum(cont_diff.any(axis=1))\n",
    "        # new continous cf\n",
    "        c_cont_hat = torch.where(cont_diff, x_cont_inv, c_cont_inv)\n",
    "        c[:, :self.cat_idx] = self.scaler.transform(c_cont_hat)\n",
    "        c_y_hat = self.predict_fn(c)\n",
    "\n",
    "        self.diffs += (torch.round(c_y) != torch.round(c_y_hat)).sum()\n",
    "\n",
    "    def compute(self):\n",
    "        return 1 - self.diffs / self.total_n_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((10, 4)) \n",
    "c = deepcopy(x)\n",
    "c[:5, :] = c[:5, :] + torch.rand((5, 4))\n",
    "c[5:, :] = c[5:, :] + torch.tensor([1.1, -2.1, 1.01, -1.2])\n",
    "\n",
    "pred_func = lambda arr: torch.mean(arr, dim=1) * 10\n",
    "\n",
    "scaler = StandardScaler().fit(x)\n",
    "c_y = pred_func(scaler.transform(c))\n",
    "\n",
    "\n",
    "sensitivity = SensitivityMetric(predict_fn=pred_func, scaler=scaler, cat_idx=4, threshold=1.)\n",
    "sensitivity.update(scaler.transform(x), scaler.transform(c), c_y)\n",
    "score = sensitivity.compute()\n",
    "diffs = sensitivity.diffs\n",
    "total_n_changes = sensitivity.total_n_changes\n",
    "\n",
    "assert torch.equal(score, torch.tensor(0.))\n",
    "assert torch.equal(diffs, torch.tensor(5))\n",
    "assert torch.equal(total_n_changes, torch.tensor(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((10, 4)) \n",
    "c = x + torch.tensor([1.1, 0, 0, -1.1])\n",
    "c[:, 1:3] = c[:, 1:3] + torch.rand((10, 2))\n",
    "\n",
    "pred_func = lambda x: torch.mean(x, dim=1) * 10\n",
    "c_y = pred_func(scaler.transform(c))\n",
    "scaler = StandardScaler().fit(x)\n",
    "\n",
    "sensitivity = SensitivityMetric(predict_fn=pred_func, scaler=scaler, cat_idx=4, threshold=1.)\n",
    "sensitivity.update(scaler.transform(x), scaler.transform(c), c_y)\n",
    "score = sensitivity.compute()\n",
    "diffs = sensitivity.diffs\n",
    "total_n_changes = sensitivity.total_n_changes\n",
    "\n",
    "assert torch.equal(score, torch.tensor(0.))\n",
    "assert torch.equal(diffs, torch.tensor(10))\n",
    "assert torch.equal(total_n_changes, torch.tensor(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export evaluation\n",
    "def proximity(x:torch.Tensor, c: torch.Tensor):\n",
    "    return torch.abs(x - c).sum(dim=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1, 2, 1])\n",
    "c = torch.tensor([-1, 1., 0.1])\n",
    "assert proximity(x, c) == torch.tensor(3.9)\n",
    "\n",
    "x_ = torch.tensor([1.5, 2.5, 1])\n",
    "c_ = torch.tensor([-0.5, 1.5, 0.1])\n",
    "assert proximity(x, c) == torch.tensor(3.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1, 2, 1], [-1, 1., 0.1]])\n",
    "c = torch.tensor([[-1, 1., 0.1], [1, 2, 1]])\n",
    "assert proximity(x, c) == torch.tensor(3.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export evaluation\n",
    "class ProximityMetric(Metric):\n",
    "    def __init__(self):\n",
    "        super().__init__(dist_sync_on_step=False)\n",
    "        self.add_state(\"dist\", default=torch.tensor(0.), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"n\", default=torch.tensor(0.), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, x, c):\n",
    "        self.dist += proximity(x, c)\n",
    "        self.n += 1\n",
    "\n",
    "    def compute(self):\n",
    "        if self.n == 0:\n",
    "            return -1\n",
    "        else:\n",
    "            return self.dist / self.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = ProximityMetric()\n",
    "x = torch.tensor([1, 2, 1])\n",
    "c = torch.tensor([-1, 1., 0.1])\n",
    "\n",
    "metric.update(x, c)\n",
    "assert metric.compute() == torch.tensor(3.9)\n",
    "\n",
    "x_ = torch.tensor([1.5, 2.5, 1])\n",
    "c_ = torch.tensor([-0.5, 1.5, 0.1])\n",
    "metric.update(x_, c_)\n",
    "assert metric.compute() == torch.tensor(3.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = ProximityMetric()\n",
    "x = torch.tensor([[1, 2, 1], [-1, 1., 0.1]])\n",
    "c = torch.tensor([[-1, 1., 0.1], [1, 2, 1]])\n",
    "metric.update(x, c)\n",
    "\n",
    "assert metric.compute() == torch.tensor(3.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define other utility functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export utils.functional\n",
    "def l1_mean(x, c):\n",
    "    return F.l1_loss(x, c, reduction='mean') / x.abs().mean() # MAD\n",
    "\n",
    "def get_loss_functions(f_name: str):\n",
    "    _loss_functions = {\n",
    "        'cross_entropy': F.binary_cross_entropy,\n",
    "        'l1': F.l1_loss,\n",
    "        'l1_mean': l1_mean,\n",
    "        'mse': F.mse_loss\n",
    "    }\n",
    "\n",
    "    assert f_name in _loss_functions.keys(), \\\n",
    "        f'function name `{f_name}` is not in the loss function list {_loss_functions.keys()}'\n",
    "\n",
    "    return _loss_functions[f_name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export utils.functional\n",
    "def split_X_y(data: pd.DataFrame):\n",
    "    X = data[data.columns[:-1]]\n",
    "    y = data[data.columns[-1]]\n",
    "    return X, y\n",
    "\n",
    "def train_val_test_split(X, y):\n",
    "    assert len(X) == len(y)\n",
    "    size = len(X)\n",
    "    train_size = int(0.7 * size)    # 70% for training\n",
    "    val_size = int(0.8 * size)      # 10% for validation\n",
    "\n",
    "    return tuple(\n",
    "        tuple(X[: train_size], y[: train_size]),\n",
    "        tuple(X[train_size:val_size], y[train_size:val_size]),\n",
    "        tuple(X[val_size:], y[val_size:])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ABCBaseModule(ABC):\n",
    "    @abstractmethod\n",
    "    def model_forward(self, *x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, *x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, *x):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class BaseModule(pl.LightningModule, ABCBaseModule):\n",
    "    def __init__(self, configs: Dict[str, Any]):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(configs)\n",
    "\n",
    "        # read data\n",
    "        self.data = pd.read_csv(Path(configs['data_dir']))\n",
    "        self.continous_cols = configs['continous_cols']\n",
    "        self.discret_cols = configs['discret_cols']\n",
    "        self.__check_cols()\n",
    "\n",
    "        # set training configs\n",
    "        self.lr = configs['lr']\n",
    "        self.batch_size = configs['batch_size']\n",
    "        self.dropout = configs['dropout'] if 'dropout' in configs.keys() else 0.3\n",
    "        self.lambda_1 = configs['lambda_1'] if 'lambda_1' in configs.keys() else 1\n",
    "        self.lambda_2 = configs['lambda_2'] if 'lambda_2' in configs.keys() else 1\n",
    "        self.lambda_3 = configs['lambda_3'] if 'lambda_3' in configs.keys() else 1\n",
    "        self.threshold = configs['threshold'] if 'threshold' in configs.keys() else 1\n",
    "        self.smooth_y = configs['smooth_y'] if 'smooth_y' in configs.keys() else True\n",
    "\n",
    "        # loss functions\n",
    "        self.loss_func_1 = get_loss_functions(configs['loss_1']) if 'loss_1' in configs.keys() else get_loss_functions(\"mse\")\n",
    "        self.loss_func_2 = get_loss_functions(configs['loss_2']) if 'loss_2' in configs.keys() else get_loss_functions(\"mse\")\n",
    "        self.loss_func_3 = get_loss_functions(configs['loss_3']) if 'loss_3' in configs.keys() else get_loss_functions(\"mse\")\n",
    "\n",
    "        # set model configss\n",
    "        self.enc_dims = configs['encoder_dims'] if 'encoder_dims' in configs.keys() else []\n",
    "        self.dec_dims = configs['decoder_dims'] if 'decoder_dims' in configs.keys() else []\n",
    "        self.exp_dims = configs['explainer_dims'] if 'explainer_dims' in configs.keys() else []\n",
    "\n",
    "        # log graph\n",
    "        self.example_input_array = torch.randn((1, self.enc_dims[0]))\n",
    "\n",
    "    def __check_cols(self):\n",
    "        assert sorted(list(self.data.columns)) == sorted(self.continous_cols + self.discret_cols)\n",
    "        self.data = self.data.astype({col: np.float for col in self.continous_cols})\n",
    "\n",
    "    def training_epoch_end(self, outs):\n",
    "        if self.current_epoch == 0:\n",
    "            self.logger.log_hyperparams(self.hparams)\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # TODO Decouple data preparision and use `LightningDataModule`\n",
    "        # 70% for training, 10% for validation, 20% for testing\n",
    "        X, y = split_X_y(self.data)\n",
    "\n",
    "        # preprocessing\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.ohe = OneHotEncoder()\n",
    "        X_cont = self.scaler.fit_transform(X[self.continous_cols]) if self.continous_cols else np.array([[] for _ in range(len(X))])\n",
    "        X_cat = self.ohe.fit_transform(X[self.discret_cols]) if self.discret_cols else np.array([[] for _ in range(len(X))])\n",
    "        X = torch.cat((X_cont, X_cat), dim=1)\n",
    "\n",
    "        # init categorical normalizer to enable categorical features to be one-hot-encoding format\n",
    "        cat_arrays = self.ohe.categories_ if self.discret_cols else []\n",
    "        self.cat_normalizer = CategoricalNormalizer(cat_arrays, cat_idx=len(X_cont))\n",
    "\n",
    "        # init sensitivity metric\n",
    "        self.sensitivity = SensitivityMetric(\n",
    "            predict_fn=self.predict, scaler=self.scaler, cat_idx=len(X_cont), threshold=self.threshold)\n",
    "\n",
    "        pl_logger.info(f\"x_cont: {X_cont.size()}, x_cat: {X_cat.size()}\")\n",
    "        pl_logger.info(\"X shape: \", X.size())\n",
    "\n",
    "        assert X.size(-1) == self.enc_dims[0],\\\n",
    "            f'The input dimension X (shape: {X.shape[-1]})  != encoder_dims[0]: {self.enc_dims}'\n",
    "\n",
    "        # prepare train & test\n",
    "        train, val, test = train_val_test_split(X, y.to_numpy())\n",
    "        self.train_dataset = NumpyDataset(*train)\n",
    "        self.val_dataset = NumpyDataset(*val)\n",
    "        self.test_dataset = NumpyDataset(*test)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size,\n",
    "                          pin_memory=True, shuffle=True, num_workers=0)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size,\n",
    "                          pin_memory=True, shuffle=False, num_workers=0)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size,\n",
    "                          pin_memory=True, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export utils.functional\n",
    "def uniform(shape: tuple, r1: float, r2: float, device=None):\n",
    "    assert r1 < r2, f\"Issue: r1 ({r1}) >= r2 ({r2})\"\n",
    "    return (r2 - r1) * torch.rand(*shape, device=device) + r1\n",
    "\n",
    "\n",
    "def smooth_y(y, device=None):\n",
    "    return torch.where(y == 1,\n",
    "        uniform(y.size(), 0.8, 0.95, device=y.device),\n",
    "        uniform(y.size(), 0.05, 0.2, device=y.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class PredictiveTrainingModule(BaseModule):\n",
    "    def __init__(self, configs: Dict[str, Any]):\n",
    "        super().__init__(configs)\n",
    "        # define metrics\n",
    "        self.val_acc = Accuracy()\n",
    "\n",
    "    def forward(self, *x):\n",
    "        return self.model_forward(x)\n",
    "\n",
    "    def predict(self, x):\n",
    "        self.freeze()\n",
    "        y_hat = self(x)\n",
    "        return torch.round(y_hat)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam([p for p in self.parameters() if p.requires_grad], lr=self.lr)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # batch\n",
    "        *x, y = batch\n",
    "        # fwd\n",
    "        y_hat = self(*x)\n",
    "        # loss\n",
    "        if self.smooth_y:\n",
    "            y = smooth_y(y)\n",
    "        loss = F.binary_cross_entropy(y_hat, y)\n",
    "\n",
    "        # Logging to TensorBoard\n",
    "        self.log('train/train_loss_1', loss, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # batch\n",
    "        *x, y = batch\n",
    "        # fwd\n",
    "        y_hat = self(*x)\n",
    "        # loss\n",
    "        loss = F.binary_cross_entropy(y_hat, y)\n",
    "        self.accuracy(y_hat, y)\n",
    "        self.log('val/val_loss_1', loss, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        self.log('val/pred_accuracy', self.accuracy, on_step=False, on_epoch=True, prog_bar=False, logger=True, sync_dist=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CounterNet Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class CFNetTrainingModule(BaseModule):\n",
    "    def __init__(self, configs: Dict[str, Any]):\n",
    "        super().__init__(configs)\n",
    "        # define metrics\n",
    "        self.pred_acc = Accuracy()\n",
    "        self.cf_acc = Accuracy()\n",
    "        self.proximity = ProximityMetric()\n",
    "\n",
    "    def forward(self, x, hard: bool=False):\n",
    "        \"\"\"hard: categorical features in counterfactual is one-hot-encoding or not\"\"\"\n",
    "        y, c = self.model_forward(x)\n",
    "        c = self.cat_normalize(c, hard=hard)\n",
    "        return y, c\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"x has not been preprocessed\"\"\"\n",
    "        self.freeze()\n",
    "        y_hat, c = self.model_forward(x)\n",
    "        return torch.round(y_hat)\n",
    "\n",
    "    def generate_cf(self, x, clamp=False):\n",
    "        self.freeze()\n",
    "        y, c = self.model_forward(x)\n",
    "        if clamp:\n",
    "            c = torch.clamp(c, 0., 1.)\n",
    "        return self.cat_normalizer.normalize(c, hard=True)\n",
    "\n",
    "    def _logging_loss(self, *loss, stage: str, on_step: bool = False):\n",
    "        for i, l in enumerate(loss):\n",
    "            self.log(f'{stage}/{stage}_loss_{i+1}', l, on_step=on_step, on_epoch=True, prog_bar=False, logger=True, sync_dist=True)\n",
    "\n",
    "    def _loss_functions(self, x, c, y, y_hat, y_prime=None, is_val=False):\n",
    "        \"\"\"\n",
    "        x: input value\n",
    "        c: conterfactual example\n",
    "        y: ground truth\n",
    "        y_hat: predicted result\n",
    "        y_prime_mode: 'label' or 'predicted'\n",
    "        \"\"\"\n",
    "        # flip zero/one\n",
    "        if y_prime == None:\n",
    "            y_prime = (y_hat < .5).clone().detach().float()\n",
    "\n",
    "        c_y, _ = self(c)\n",
    "        # loss functions\n",
    "        if self.smooth_y and not is_val:\n",
    "            y = smooth_y(y)\n",
    "            y_prime = smooth_y(y_prime)\n",
    "        l_1 = self.loss_func_1(y_hat, y)\n",
    "        l_2 = self.loss_func_2(x, c)\n",
    "        l_3 = self.loss_func_3(c_y, y_prime)\n",
    "\n",
    "        return l_1, l_2, l_3\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt_1 = torch.optim.Adam([p for p in self.parameters() if p.requires_grad], lr=self.lr)\n",
    "        opt_2 = torch.optim.Adam([p for p in self.parameters() if p.requires_grad], lr=self.lr)\n",
    "        return (opt_1, opt_2)\n",
    "\n",
    "    def predictor_step(self, l_1, l_3):\n",
    "        p_loss = self.lambda_1 * l_1 # + self.lambda_3 * l_3\n",
    "        self.log('train/p_loss', p_loss, on_step=False, on_epoch=True, sync_dist=True)\n",
    "        return p_loss\n",
    "\n",
    "    def explainer_step(self, l_2, l_3):\n",
    "        e_loss = self.lambda_2 * l_2 + self.lambda_3 * l_3\n",
    "        self.log('train/e_loss', e_loss, on_step=False, on_epoch=True, sync_dist=True)\n",
    "        return e_loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        # batch\n",
    "        x, y = batch\n",
    "        # fwd\n",
    "        y_hat, c = self(x)\n",
    "        # loss\n",
    "        l_1, l_2, l_3 = self._loss_functions(x, c, y, y_hat)\n",
    "\n",
    "        result = 0\n",
    "        if optimizer_idx == 0:\n",
    "            result = self.predictor_step(l_1, l_3)\n",
    "\n",
    "        if optimizer_idx == 1:\n",
    "            result = self.explainer_step(l_2, l_3)\n",
    "\n",
    "        # Logging to TensorBoard by default\n",
    "        self._logging_loss(l_1, l_2, l_3, stage='train', on_step=False)\n",
    "        return result\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # batch\n",
    "        x, y = batch\n",
    "\n",
    "        # fwd\n",
    "        y_hat, c = self(x, hard=True)\n",
    "        c_y, _ = self(c)\n",
    "\n",
    "        # loss\n",
    "        l_1, l_2, l_3 = self._loss_functions(x, c, y, y_hat, is_val=True)\n",
    "        loss = self.predict_step(l_1, l_3) + self.explainer_step(l_2, l_3)\n",
    "\n",
    "        # logging val loss\n",
    "        self._logging_loss(l_1, l_2, l_3, stage='val', on_step=False)\n",
    "\n",
    "        # metrics\n",
    "        metrics = {\n",
    "            'val/val_loss': loss, 'val/pred_accuracy': self.pred_acc(round(y_hat), y),\n",
    "            'val/cf_proximity': self.proximity(x, c), 'val/cf_accuracy': self.cf_acc(round(c_y), 1 - round(y_hat)),\n",
    "            'val/sensitivity': self.sensitivity(x, c, c_y),\n",
    "        }\n",
    "        self.log_dict(metrics, on_step=False, on_epoch=True, sync_dist=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export model\n",
    "class LinearBlock(nn.Module):\n",
    "    def __init__(self, input_dim, out_dim, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(input_dim, out_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class MultilayerPerception(nn.Module):\n",
    "    def __init__(self, dims=[3, 100, 10], dropout=0.3):\n",
    "        super().__init__()\n",
    "        layers  = []\n",
    "        num_blocks = len(dims)\n",
    "        for i in range(1, num_blocks):\n",
    "            layers += [\n",
    "                LinearBlock(dims[i-1], dims[i], dropout=dropout)\n",
    "            ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export model\n",
    "class BaselinePredictiveModel(PredictiveTrainingModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        assert self.enc_dims[-1] == self.dec_dims[0], \\\n",
    "            f\"(enc_dims[-1]={self.enc_dims[-1]}) != (dec_dims[0]={self.dec_dims[0]})\"\n",
    "        self.model = nn.Sequential(\n",
    "            MultilayerPerception(self.enc_dims, self.dropout),\n",
    "            MultilayerPerception(self.dec_dims, self.dropout),\n",
    "            nn.Linear(self.dec_dims[-1], 1)\n",
    "        )\n",
    "\n",
    "    def model_forward(self, x):\n",
    "        # x = ([],)\n",
    "        x, = x\n",
    "        y_hat = torch.sigmoid(self.model(x))\n",
    "        return torch.squeeze(y_hat, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export model\n",
    "class CounterNetModel(CFNetTrainingModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        assert self.enc_dims[-1] == self.dec_dims[0], \\\n",
    "            f\"(enc_dims[-1]={self.enc_dims[-1]}) != (dec_dims[0]={self.dec_dims[0]})\"\n",
    "        assert self.enc_dims[-1] == self.exp_dims[0], \\\n",
    "            f\"(enc_dims[-1]={self.enc_dims[-1]}) != (exp_dims[0]={self.enc_dims[0]})\"\n",
    "\n",
    "        self.encoder_model = MultilayerPerception(self.enc_dims)\n",
    "        # predictor\n",
    "        self.predictor = MultilayerPerception(self.dec_dims)\n",
    "        self.pred_linear = nn.Linear(self.dec_dims[-1], 1)\n",
    "        # explainer\n",
    "        exp_dims = [x for x in self.exp_dims]\n",
    "        exp_dims[0] = self.exp_dims[0] + self.dec_dims[-1]\n",
    "\n",
    "        self.explainer = nn.Sequential(\n",
    "            MultilayerPerception(exp_dims),\n",
    "            nn.Linear(self.exp_dims[-1], self.enc_dims[0])\n",
    "        )\n",
    "\n",
    "    def model_forward(self, x):\n",
    "        x = self.encoder_model(x)\n",
    "        # predicted y_hat\n",
    "        pred = self.predictor(x)\n",
    "        y_hat = torch.sigmoid(self.pred_linear(pred))\n",
    "        # counterfactual example\n",
    "        x = torch.cat((x, pred), -1)\n",
    "        c = self.explainer(x)\n",
    "        return torch.squeeze(y_hat, -1), c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
