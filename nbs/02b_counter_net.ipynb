{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp training_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from counternet.import_essentials import *\n",
    "from counternet.utils import *\n",
    "from counternet.evaluation import SensitivityMetric, ProximityMetric\n",
    "from counternet.cf_explainer import GlobalExplainerBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pl version: 1.3.5\n",
      "torch version: 1.8.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"pl version: {pl.__version__}\")\n",
    "print(f\"torch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 818 ms, sys: 20.3 ms, total: 839 ms\n",
      "Wall time: 837 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dummy_data = pd.read_csv('assets/data/dummy_data.csv')\n",
    "adult_data = load_adult_income_dataset('assets/data/adult.data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the categorical elements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export processing\n",
    "class CategoricalNormalizer(object):\n",
    "    \"\"\"implement post-processing step to enforce each elements\n",
    "    in every category in the range of [0, 1] and output to 1.\n",
    "    \"\"\"\n",
    "    def __init__(self, categories: List[List[Any]], cat_idx: int):\n",
    "        self.categories = categories\n",
    "        self.cat_idx = cat_idx\n",
    "\n",
    "    def normalize(self, x, hard=False):\n",
    "        cat_idx = self.cat_idx\n",
    "        for col in self.categories:\n",
    "            cat_end_idx = cat_idx + len(col)\n",
    "            if hard:\n",
    "                x[:, cat_idx: cat_end_idx] = F.gumbel_softmax(x[:, cat_idx: cat_end_idx].clone().detach(), hard=hard)\n",
    "            else:\n",
    "                x[:, cat_idx: cat_end_idx] = F.softmax(x[:, cat_idx: cat_end_idx].clone().detach(), dim=-1)\n",
    "            cat_idx = cat_end_idx\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    ['a', 'b', 'c'],\n",
    "    ['e', 'f'],\n",
    "    [1, 2]\n",
    "]\n",
    "X = [[categories[_j][np.random.randint(0, len(categories[_j]))] for _j in range(3)] for _i in range(100)]\n",
    "X_ = OneHotEncoder().fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export evaluation\n",
    "class SensitivityMetric(Metric):\n",
    "    def __init__(self, predict_fn: Callable, scaler: ABCScaler, cat_idx: int, threshold: float):\n",
    "        super().__init__(dist_sync_on_step=False)\n",
    "        self.predict_fn = predict_fn\n",
    "        self.scaler = scaler\n",
    "        self.cat_idx = cat_idx\n",
    "        self.threshold = threshold\n",
    "\n",
    "        self.add_state(\"total_n_changes\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"diffs\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, x: torch.Tensor, c: torch.Tensor, c_y: torch.Tensor):\n",
    "        # inverse transform\n",
    "        x_cont_inv = self.scaler.inverse_transform(x[:, :self.cat_idx])\n",
    "        c_cont_inv = self.scaler.inverse_transform(c[:, :self.cat_idx])\n",
    "        # a bool metrics on whether differences between x and c is smaller than the threshold\n",
    "        cont_diff = torch.abs(x_cont_inv - c_cont_inv) < self.threshold\n",
    "        # total nums of differences\n",
    "        self.total_n_changes += torch.sum(cont_diff.any(axis=1))\n",
    "        # new continous cf\n",
    "        c_cont_hat = torch.where(cont_diff, x_cont_inv, c_cont_inv)\n",
    "        c[:, :self.cat_idx] = self.scaler.transform(c_cont_hat)\n",
    "        c_y_hat = self.predict_fn(c)\n",
    "\n",
    "        self.diffs += (torch.round(c_y) != torch.round(c_y_hat)).sum()\n",
    "\n",
    "    def compute(self):\n",
    "        return 1 - self.diffs / self.total_n_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((10, 4)) \n",
    "c = deepcopy(x)\n",
    "c[:5, :] = c[:5, :] + torch.rand((5, 4))\n",
    "c[5:, :] = c[5:, :] + torch.tensor([1.1, -2.1, 1.01, -1.2])\n",
    "\n",
    "pred_func = lambda arr: torch.mean(arr, dim=1) * 10\n",
    "\n",
    "scaler = StandardScaler().fit(x)\n",
    "c_y = pred_func(scaler.transform(c))\n",
    "\n",
    "\n",
    "sensitivity = SensitivityMetric(predict_fn=pred_func, scaler=scaler, cat_idx=4, threshold=1.)\n",
    "sensitivity.update(scaler.transform(x), scaler.transform(c), c_y)\n",
    "score = sensitivity.compute()\n",
    "diffs = sensitivity.diffs\n",
    "total_n_changes = sensitivity.total_n_changes\n",
    "\n",
    "assert torch.equal(score, torch.tensor(0.))\n",
    "assert torch.equal(diffs, torch.tensor(5))\n",
    "assert torch.equal(total_n_changes, torch.tensor(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((10, 4)) \n",
    "c = x + torch.tensor([1.1, 0, 0, -1.1])\n",
    "c[:, 1:3] = c[:, 1:3] + torch.rand((10, 2))\n",
    "\n",
    "pred_func = lambda x: torch.mean(x, dim=1) * 10\n",
    "c_y = pred_func(scaler.transform(c))\n",
    "scaler = StandardScaler().fit(x)\n",
    "\n",
    "sensitivity = SensitivityMetric(predict_fn=pred_func, scaler=scaler, cat_idx=4, threshold=1.)\n",
    "sensitivity.update(scaler.transform(x), scaler.transform(c), c_y)\n",
    "score = sensitivity.compute()\n",
    "diffs = sensitivity.diffs\n",
    "total_n_changes = sensitivity.total_n_changes\n",
    "\n",
    "assert torch.equal(score, torch.tensor(0.))\n",
    "assert torch.equal(diffs, torch.tensor(10))\n",
    "assert torch.equal(total_n_changes, torch.tensor(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export evaluation\n",
    "def proximity(x:torch.Tensor, c: torch.Tensor):\n",
    "    return torch.abs(x - c).sum(dim=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1, 2, 1])\n",
    "c = torch.tensor([-1, 1., 0.1])\n",
    "assert proximity(x, c) == torch.tensor(3.9)\n",
    "\n",
    "x_ = torch.tensor([1.5, 2.5, 1])\n",
    "c_ = torch.tensor([-0.5, 1.5, 0.1])\n",
    "assert proximity(x, c) == torch.tensor(3.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1, 2, 1], [-1, 1., 0.1]])\n",
    "c = torch.tensor([[-1, 1., 0.1], [1, 2, 1]])\n",
    "assert proximity(x, c) == torch.tensor(3.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export evaluation\n",
    "class ProximityMetric(Metric):\n",
    "    def __init__(self):\n",
    "        super().__init__(dist_sync_on_step=False)\n",
    "        self.add_state(\"dist\", default=torch.tensor(0.), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"n\", default=torch.tensor(0.), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, x, c):\n",
    "        self.dist += proximity(x, c)\n",
    "        self.n += 1\n",
    "\n",
    "    def compute(self):\n",
    "        if self.n == 0:\n",
    "            return -1\n",
    "        else:\n",
    "            return self.dist / self.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = ProximityMetric()\n",
    "x = torch.tensor([1, 2, 1])\n",
    "c = torch.tensor([-1, 1., 0.1])\n",
    "\n",
    "metric.update(x, c)\n",
    "assert metric.compute() == torch.tensor(3.9)\n",
    "\n",
    "x_ = torch.tensor([1.5, 2.5, 1])\n",
    "c_ = torch.tensor([-0.5, 1.5, 0.1])\n",
    "metric.update(x_, c_)\n",
    "assert metric.compute() == torch.tensor(3.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = ProximityMetric()\n",
    "x = torch.tensor([[1, 2, 1], [-1, 1., 0.1]])\n",
    "c = torch.tensor([[-1, 1., 0.1], [1, 2, 1]])\n",
    "metric.update(x, c)\n",
    "\n",
    "assert metric.compute() == torch.tensor(3.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define other utility functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export functional_utils\n",
    "def l1_mean(x, c):\n",
    "    return F.l1_loss(x, c, reduction='mean') / x.abs().mean() # MAD\n",
    "\n",
    "def get_loss_functions(f_name: str):\n",
    "    _loss_functions = {\n",
    "        'cross_entropy': F.binary_cross_entropy,\n",
    "        'l1': F.l1_loss,\n",
    "        'l1_mean': l1_mean,\n",
    "        'mse': F.mse_loss\n",
    "    }\n",
    "\n",
    "    assert f_name in _loss_functions.keys(), \\\n",
    "        f'function name `{f_name}` is not in the loss function list {_loss_functions.keys()}'\n",
    "\n",
    "    return _loss_functions[f_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export functional_utils\n",
    "def split_X_y(data: pd.DataFrame):\n",
    "    X = data[data.columns[:-1]]\n",
    "    y = data[data.columns[-1]]\n",
    "    return X, y\n",
    "\n",
    "@check_input_type\n",
    "def train_val_test_split(X, y):\n",
    "    assert len(X) == len(y)\n",
    "    size = len(X)\n",
    "    train_size = int(0.7 * size)    # 70% for training\n",
    "    val_size = int(0.8 * size)      # 10% for validation\n",
    "\n",
    "    return (\n",
    "        (X[: train_size], y[: train_size]),\n",
    "        (X[train_size:val_size], y[train_size:val_size]),\n",
    "        (X[val_size:], y[val_size:])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('assets/data/dummy_data.csv')\n",
    "X, y = split_X_y(data)\n",
    "_ = train_val_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ABCBaseModule(ABC):\n",
    "    @abstractmethod\n",
    "    def model_forward(self, *x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, *x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, *x):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class BaseModule(pl.LightningModule, ABCBaseModule):\n",
    "    def __init__(self, configs: Dict[str, Any]):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(configs)\n",
    "\n",
    "        # read data\n",
    "        self.data = pd.read_csv(Path(configs['data_dir']))\n",
    "        self.continous_cols = configs['continous_cols']\n",
    "        self.discret_cols = configs['discret_cols']\n",
    "        self.__check_cols()\n",
    "\n",
    "        # set training configs\n",
    "        self.lr = configs['lr']\n",
    "        self.batch_size = configs['batch_size']\n",
    "        self.dropout = configs['dropout'] if 'dropout' in configs.keys() else 0.3\n",
    "        self.lambda_1 = configs['lambda_1'] if 'lambda_1' in configs.keys() else 1\n",
    "        self.lambda_2 = configs['lambda_2'] if 'lambda_2' in configs.keys() else 1\n",
    "        self.lambda_3 = configs['lambda_3'] if 'lambda_3' in configs.keys() else 1\n",
    "        self.threshold = configs['threshold'] if 'threshold' in configs.keys() else 1\n",
    "        self.smooth_y = configs['smooth_y'] if 'smooth_y' in configs.keys() else True\n",
    "\n",
    "        # loss functions\n",
    "        self.loss_func_1 = get_loss_functions(configs['loss_1']) if 'loss_1' in configs.keys() else get_loss_functions(\"mse\")\n",
    "        self.loss_func_2 = get_loss_functions(configs['loss_2']) if 'loss_2' in configs.keys() else get_loss_functions(\"mse\")\n",
    "        self.loss_func_3 = get_loss_functions(configs['loss_3']) if 'loss_3' in configs.keys() else get_loss_functions(\"mse\")\n",
    "\n",
    "        # set model configss\n",
    "        self.enc_dims = configs['encoder_dims'] if 'encoder_dims' in configs.keys() else []\n",
    "        self.dec_dims = configs['decoder_dims'] if 'decoder_dims' in configs.keys() else []\n",
    "        self.exp_dims = configs['explainer_dims'] if 'explainer_dims' in configs.keys() else []\n",
    "\n",
    "        # log graph\n",
    "        self.example_input_array = torch.randn((1, self.enc_dims[0]))\n",
    "\n",
    "    def __check_cols(self):\n",
    "        assert sorted(list(self.data.columns[:-1])) == sorted(self.continous_cols + self.discret_cols), \\\n",
    "            f\"data columns ({sorted(list(self.data.columns[:-1]))}) is not the same as continous_cols and discret_cols ({sorted(self.continous_cols + self.discret_cols)})\"\n",
    "        self.data = self.data.astype(\n",
    "            {col: np.float for col in self.continous_cols})\n",
    "\n",
    "    def __check_cat_size(self, X_cat: torch.Tensor, categories: List[List[Any]]):\n",
    "        n = 0\n",
    "        for cat in categories:\n",
    "            n += len(cat)\n",
    "        assert X_cat.size(-1) == n\n",
    "\n",
    "    def training_epoch_end(self, outs):\n",
    "        if self.current_epoch == 0:\n",
    "            self.logger.log_hyperparams(self.hparams)\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # TODO Decouple data preparision and use `LightningDataModule`\n",
    "        # 70% for training, 10% for validation, 20% for testing\n",
    "        X, y = split_X_y(self.data)\n",
    "\n",
    "        # preprocessing\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.ohe = OneHotEncoder()\n",
    "        X_cont = self.scaler.fit_transform(X[self.continous_cols]) if self.continous_cols else torch.tensor([[] for _ in range(len(X))])\n",
    "        X_cat = self.ohe.fit_transform(X[self.discret_cols]) if self.discret_cols else torch.tensor([[] for _ in range(len(X))])\n",
    "        X = torch.cat((X_cont, X_cat), dim=1)\n",
    "\n",
    "        # init categorical normalizer to enable categorical features to be one-hot-encoding format\n",
    "        cat_arrays = self.ohe.categories_ if self.discret_cols else []\n",
    "        self.cat_normalizer = CategoricalNormalizer(cat_arrays, cat_idx=len(self.continous_cols))\n",
    "        self.__check_cat_size(X_cat, cat_arrays)\n",
    "\n",
    "        # init sensitivity metric\n",
    "        self.sensitivity = SensitivityMetric(\n",
    "            predict_fn=self.predict, scaler=self.scaler, cat_idx=len(self.continous_cols), threshold=self.threshold)\n",
    "\n",
    "        print(f\"x_cont: {X_cont.size()}, x_cat: {X_cat.size()}, X shape: {X.size()}\")\n",
    "\n",
    "        assert X.size(-1) == self.enc_dims[0],\\\n",
    "            f'The input dimension X (shape: {X.shape[-1]})  != encoder_dims[0]: {self.enc_dims}'\n",
    "\n",
    "        # prepare train & test\n",
    "        train, val, test = train_val_test_split(X, y)\n",
    "        self.train_dataset = TensorDataset(*train)\n",
    "        self.val_dataset = TensorDataset(*val)\n",
    "        self.test_dataset = TensorDataset(*test)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size,\n",
    "                          pin_memory=True, shuffle=True, num_workers=0)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size,\n",
    "                          pin_memory=True, shuffle=False, num_workers=0)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size,\n",
    "                          pin_memory=True, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export functional_utils\n",
    "def uniform(shape: tuple, r1: float, r2: float, device=None):\n",
    "    assert r1 < r2, f\"Issue: r1 ({r1}) >= r2 ({r2})\"\n",
    "    return (r2 - r1) * torch.rand(*shape, device=device) + r1\n",
    "\n",
    "\n",
    "def smooth_y(y, device=None):\n",
    "    return torch.where(y == 1,\n",
    "        uniform(y.size(), 0.8, 0.95, device=y.device),\n",
    "        uniform(y.size(), 0.05, 0.2, device=y.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class PredictiveTrainingModule(BaseModule):\n",
    "    def __init__(self, configs: Dict[str, Any]):\n",
    "        super().__init__(configs)\n",
    "        # define metrics\n",
    "        self.val_acc = Accuracy()\n",
    "\n",
    "    def forward(self, *x):\n",
    "        return self.model_forward(x)\n",
    "\n",
    "    def predict(self, x):\n",
    "        y_hat = self(x)\n",
    "        return torch.round(y_hat)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam([p for p in self.parameters() if p.requires_grad], lr=self.lr)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # batch\n",
    "        *x, y = batch\n",
    "        # fwd\n",
    "        y_hat = self(*x)\n",
    "        # loss\n",
    "        if self.smooth_y:\n",
    "            y = smooth_y(y)\n",
    "        loss = F.binary_cross_entropy(y_hat, y)\n",
    "\n",
    "        # Logging to TensorBoard\n",
    "        self.log('train/train_loss_1', loss, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # batch\n",
    "        *x, y = batch\n",
    "        # fwd\n",
    "        y_hat = self(*x)\n",
    "        # loss\n",
    "        loss = F.binary_cross_entropy(y_hat, y)\n",
    "\n",
    "        self.log('val/val_loss', loss, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        self.log('val/pred_accuracy', self.val_acc(y_hat, y.int()), on_step=False, on_epoch=True, sync_dist=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CounterNet Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export functional_utils\n",
    "@check_input_type\n",
    "def flip_binary(x):\n",
    "    assert ((x < 0) & (x > 1)).sum() == 0\n",
    "    return (1 - torch.round(x)).clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_x = torch.tensor([0.1, 0.56, 0.9, 1., 0.])\n",
    "flipped = flip_binary(_x)\n",
    "assert torch.equal(flipped, torch.tensor([1, 0, 0, 0, 1], dtype=torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class CFNetTrainingModule(BaseModule, GlobalExplainerBase):\n",
    "    def __init__(self, configs: Dict[str, Any]):\n",
    "        super().__init__(configs)\n",
    "        # define metrics\n",
    "        self.pred_acc = Accuracy()\n",
    "        self.cf_acc = Accuracy()\n",
    "        self.proximity = ProximityMetric()\n",
    "\n",
    "    def forward(self, x, hard: bool=False):\n",
    "        \"\"\"hard: categorical features in counterfactual is one-hot-encoding or not\"\"\"\n",
    "        y, c = self.model_forward(x)\n",
    "        c = self.cat_normalizer.normalize(c, hard=hard)\n",
    "        return y, c\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"x has not been preprocessed\"\"\"\n",
    "        y_hat, _ = self.model_forward(x)\n",
    "        return torch.round(y_hat)\n",
    "\n",
    "    def generate_cf(self, x, clamp=False):\n",
    "        self.freeze()\n",
    "        y, c = self.model_forward(x)\n",
    "        if clamp:\n",
    "            c = torch.clamp(c, 0., 1.)\n",
    "        return self.cat_normalizer.normalize(c, hard=True)\n",
    "\n",
    "    def _logging_loss(self, *loss, stage: str, on_step: bool = False):\n",
    "        for i, l in enumerate(loss):\n",
    "            self.log(f'{stage}/{stage}_loss_{i+1}', l, on_step=on_step, on_epoch=True, prog_bar=False, logger=True, sync_dist=True)\n",
    "\n",
    "    def _loss_functions(self, x, c, y, y_hat, y_prime=None, is_val=False):\n",
    "        \"\"\"\n",
    "        x: input value\n",
    "        c: conterfactual example\n",
    "        y: ground truth\n",
    "        y_hat: predicted result\n",
    "        y_prime_mode: 'label' or 'predicted'\n",
    "        \"\"\"\n",
    "        # flip zero/one\n",
    "        if y_prime == None:\n",
    "            y_prime = flip_binary(y_hat)\n",
    "\n",
    "        c_y, _ = self(c)\n",
    "        # loss functions\n",
    "        if self.smooth_y and not is_val:\n",
    "            y = smooth_y(y)\n",
    "            y_prime = smooth_y(y_prime)\n",
    "        l_1 = self.loss_func_1(y_hat, y)\n",
    "        l_2 = self.loss_func_2(x, c)\n",
    "        l_3 = self.loss_func_3(c_y, y_prime)\n",
    "\n",
    "        return l_1, l_2, l_3\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt_1 = torch.optim.Adam([p for p in self.parameters() if p.requires_grad], lr=self.lr)\n",
    "        opt_2 = torch.optim.Adam([p for p in self.parameters() if p.requires_grad], lr=self.lr)\n",
    "        return (opt_1, opt_2)\n",
    "\n",
    "    def predictor_step(self, l_1, l_3):\n",
    "        p_loss = self.lambda_1 * l_1 # + self.lambda_3 * l_3\n",
    "        self.log('train/p_loss', p_loss, on_step=False, on_epoch=True, sync_dist=True)\n",
    "        return p_loss\n",
    "\n",
    "    def explainer_step(self, l_2, l_3):\n",
    "        e_loss = self.lambda_2 * l_2 + self.lambda_3 * l_3\n",
    "        self.log('train/e_loss', e_loss, on_step=False, on_epoch=True, sync_dist=True)\n",
    "        return e_loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        # batch\n",
    "        x, y = batch\n",
    "        # fwd\n",
    "        y_hat, c = self(x)\n",
    "        # loss\n",
    "        l_1, l_2, l_3 = self._loss_functions(x, c, y, y_hat)\n",
    "\n",
    "        result = 0\n",
    "        if optimizer_idx == 0:\n",
    "            result = self.predictor_step(l_1, l_3)\n",
    "\n",
    "        if optimizer_idx == 1:\n",
    "            result = self.explainer_step(l_2, l_3)\n",
    "\n",
    "        # Logging to TensorBoard by default\n",
    "        self._logging_loss(l_1, l_2, l_3, stage='train', on_step=False)\n",
    "        return result\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # batch\n",
    "        x, y = batch\n",
    "\n",
    "        # fwd\n",
    "        y_hat, c = self(x, hard=True)\n",
    "        c_y, _ = self(c)\n",
    "\n",
    "        # loss\n",
    "        l_1, l_2, l_3 = self._loss_functions(x, c, y, y_hat, is_val=True)\n",
    "        loss = self.lambda_1 * l_1 + self.lambda_2 * l_2 + self.lambda_3 * l_3\n",
    "\n",
    "        # logging val loss\n",
    "        self._logging_loss(l_1, l_2, l_3, stage='val', on_step=False)\n",
    "\n",
    "        # metrics\n",
    "        metrics = {\n",
    "            'val/val_loss': loss, 'val/pred_accuracy': self.pred_acc(torch.round(y_hat), y.int()),\n",
    "            'val/cf_proximity': self.proximity(x, c), 'val/sensitivity': self.sensitivity(x, c, c_y),\n",
    "            'val/cf_accuracy': self.cf_acc(torch.round(c_y), flip_binary(y_hat).int()),\n",
    "        }\n",
    "        self.log_dict(metrics, on_step=False, on_epoch=True, sync_dist=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export model\n",
    "class LinearBlock(pl.LightningModule):\n",
    "    def __init__(self, input_dim, out_dim, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(input_dim, out_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class MultilayerPerception(pl.LightningModule):\n",
    "    def __init__(self, dims=[3, 100, 10], dropout=0.3):\n",
    "        super().__init__()\n",
    "        layers  = []\n",
    "        num_blocks = len(dims)\n",
    "        for i in range(1, num_blocks):\n",
    "            layers += [\n",
    "                LinearBlock(dims[i-1], dims[i], dropout=dropout)\n",
    "            ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export model\n",
    "class BaselinePredictiveModel(PredictiveTrainingModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        assert self.enc_dims[-1] == self.dec_dims[0], \\\n",
    "            f\"(enc_dims[-1]={self.enc_dims[-1]}) != (dec_dims[0]={self.dec_dims[0]})\"\n",
    "        self.model = nn.Sequential(\n",
    "            MultilayerPerception(self.enc_dims, self.dropout),\n",
    "            MultilayerPerception(self.dec_dims, self.dropout),\n",
    "            nn.Linear(self.dec_dims[-1], 1)\n",
    "        )\n",
    "\n",
    "    def model_forward(self, x):\n",
    "        # x = ([],)\n",
    "        x, = x\n",
    "        y_hat = torch.sigmoid(self.model(x))\n",
    "        return torch.squeeze(y_hat, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export model\n",
    "class CounterNetModel(CFNetTrainingModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        assert self.enc_dims[-1] == self.dec_dims[0]\n",
    "        assert self.enc_dims[-1] == self.exp_dims[0]\n",
    "\n",
    "        self.encoder_model = MultilayerPerception(self.enc_dims)\n",
    "        self.predictor = nn.Sequential(\n",
    "            MultilayerPerception(self.dec_dims),\n",
    "            nn.Linear(self.dec_dims[-1], 1)\n",
    "        )\n",
    "        self.explainer = nn.Sequential(\n",
    "            MultilayerPerception(self.exp_dims),\n",
    "            nn.Linear(self.exp_dims[-1], self.enc_dims[0])\n",
    "        )\n",
    "\n",
    "    def model_forward(self, x):\n",
    "        x = self.encoder_model(x)\n",
    "        # predicted y_hat\n",
    "        y_hat = torch.sigmoid(self.predictor(x))\n",
    "        # counterfactual example\n",
    "        c = self.explainer(x)\n",
    "        return torch.squeeze(y_hat, -1), c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "x_cont: torch.Size([32561, 2]), x_cat: torch.Size([32561, 27])\n",
      "categories: [array(['Government', 'Other/Unknown', 'Private', 'Self-Employed'],\n",
      "      dtype=object), array(['Assoc', 'Bachelors', 'Doctorate', 'HS-grad', 'Masters',\n",
      "       'Prof-school', 'School', 'Some-college'], dtype=object), array(['Divorced', 'Married', 'Separated', 'Single', 'Widowed'],\n",
      "      dtype=object), array(['Blue-Collar', 'Other/Unknown', 'Professional', 'Sales', 'Service',\n",
      "       'White-Collar'], dtype=object), array(['Other', 'White'], dtype=object), array(['Female', 'Male'], dtype=object)]\n",
      "X shape:  torch.Size([32561, 29])\n",
      "\n",
      "  | Name          | Type                 | Params | In sizes | Out sizes\n",
      "------------------------------------------------------------------------------\n",
      "0 | pred_acc      | Accuracy             | 0      | ?        | ?        \n",
      "1 | cf_acc        | Accuracy             | 0      | ?        | ?        \n",
      "2 | proximity     | ProximityMetric      | 0      | ?        | ?        \n",
      "3 | encoder_model | MultilayerPerception | 2.0 K  | [1, 29]  | [1, 10]  \n",
      "4 | predictor     | Sequential           | 121    | [1, 10]  | [1, 1]   \n",
      "5 | explainer     | Sequential           | 2.0 K  | [1, 10]  | [1, 29]  \n",
      "6 | sensitivity   | SensitivityMetric    | 0      | ?        | ?        \n",
      "------------------------------------------------------------------------------\n",
      "4.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.2 K     Total params\n",
      "0.017     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e463ab1c065441d8235319b44dcfb61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation sanity check'), FloatProgress(value=1.0, bar_style='info', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/birk/software/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/birk/software/miniconda3/lib/python3.8/site-packages/pytorch_lightning/core/step_result.py:115: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  value = torch.tensor(value, device=device, dtype=torch.float)\n",
      "Global seed set to 31\n",
      "/home/birk/software/miniconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1918d02aea004c17aa0c797333ae6615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad63fc51574e4cf8a53bc3f5598896c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'trainer': <pytorch_lightning.trainer.trainer.Trainer at 0x7f320a1500d0>,\n",
       " 'module': CounterNetModel(\n",
       "   (pred_acc): Accuracy()\n",
       "   (cf_acc): Accuracy()\n",
       "   (proximity): ProximityMetric()\n",
       "   (encoder_model): MultilayerPerception(\n",
       "     (model): Sequential(\n",
       "       (0): LinearBlock(\n",
       "         (block): Sequential(\n",
       "           (0): Linear(in_features=29, out_features=50, bias=True)\n",
       "           (1): LeakyReLU(negative_slope=0.01)\n",
       "           (2): Dropout(p=0.3, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (1): LinearBlock(\n",
       "         (block): Sequential(\n",
       "           (0): Linear(in_features=50, out_features=10, bias=True)\n",
       "           (1): LeakyReLU(negative_slope=0.01)\n",
       "           (2): Dropout(p=0.3, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (predictor): Sequential(\n",
       "     (0): MultilayerPerception(\n",
       "       (model): Sequential(\n",
       "         (0): LinearBlock(\n",
       "           (block): Sequential(\n",
       "             (0): Linear(in_features=10, out_features=10, bias=True)\n",
       "             (1): LeakyReLU(negative_slope=0.01)\n",
       "             (2): Dropout(p=0.3, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (1): Linear(in_features=10, out_features=1, bias=True)\n",
       "   )\n",
       "   (explainer): Sequential(\n",
       "     (0): MultilayerPerception(\n",
       "       (model): Sequential(\n",
       "         (0): LinearBlock(\n",
       "           (block): Sequential(\n",
       "             (0): Linear(in_features=10, out_features=50, bias=True)\n",
       "             (1): LeakyReLU(negative_slope=0.01)\n",
       "             (2): Dropout(p=0.3, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (1): Linear(in_features=50, out_features=29, bias=True)\n",
       "   )\n",
       "   (sensitivity): SensitivityMetric()\n",
       " )}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from counternet.pipeline import ModelTrainer\n",
    "\n",
    "t_config = load_configs('assets/configs/trainer.json')\n",
    "m_config = load_configs('assets/configs/adult.json')\n",
    "model = CounterNetModel(m_config)\n",
    "model_trainer = ModelTrainer(\n",
    "    model, t_config\n",
    ")\n",
    "model_trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
