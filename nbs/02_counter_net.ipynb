{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp counternet_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 31\n"
     ]
    }
   ],
   "source": [
    "# export\n",
    "from counternet.import_essentials import *\n",
    "from counternet.utils.all import *\n",
    "from counternet.evaluation import SensitivityMetric\n",
    "\n",
    "\n",
    "pl_logger = logging.getLogger('lightning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pl version: 1.3.5\n",
      "torch version: 1.8.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"pl version: {pl.__version__}\")\n",
    "print(f\"torch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 768 ms, sys: 151 ms, total: 919 ms\n",
      "Wall time: 915 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dummy_data = pd.read_csv('assets/data/dummy_data.csv')\n",
    "adult_data = load_adult_income_dataset('assets/data/adult.data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export utils.processing\n",
    "class CategoricalNormalizer(object):\n",
    "    \"\"\"implement post-processing step to enforce each elements \n",
    "    in every category in the range of [0, 1] and output to 1.\n",
    "    \"\"\"\n",
    "    def __init__(self, categories: List[List[Any]], cat_idx: int):\n",
    "        self.categories = categories\n",
    "        self.cat_idx = cat_idx\n",
    "\n",
    "    def normalize(self, x, hard=False):\n",
    "        cat_idx = self.cat_idx\n",
    "        for col in self.categories:\n",
    "            cat_end_idx = cat_idx + len(col)\n",
    "            if hard:\n",
    "                x[:, cat_idx: cat_end_idx] = F.gumbel_softmax(x[:, cat_idx: cat_end_idx].clone().detach(), hard=hard)\n",
    "            else:\n",
    "                x[:, cat_idx: cat_end_idx] = F.softmax(x[:, cat_idx: cat_end_idx].clone().detach(), dim=-1)\n",
    "            cat_idx = cat_end_idx\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export evaluation\n",
    "class SensitivityMetric(Metric):\n",
    "    def __init__(self, predict_fn: Callable, scaler: ABCScaler, cat_idx: int, threshold: float):\n",
    "        super().__init__(dist_sync_on_step=False)\n",
    "        self.predict_fn = predict_fn\n",
    "        self.scaler = scaler\n",
    "        self.cat_idx = cat_idx\n",
    "        self.threshold = threshold\n",
    "\n",
    "        self.add_state(\"total_n_changes\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"diffs\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, x: torch.Tensor, c: torch.Tensor, c_y: torch.Tensor):\n",
    "        # inverse transform\n",
    "        x_cont_inv = self.scaler.inverse_transform(x[:, :self.cat_idx])\n",
    "        c_cont_inv = self.scaler.inverse_transform(c[:, :self.cat_idx])\n",
    "        # a bool metrics on whether differences between x and c is smaller than the threshold\n",
    "        cont_diff = torch.abs(x_cont_inv - c_cont_inv) < self.threshold\n",
    "        # total nums of differences\n",
    "        self.total_n_changes += torch.sum(cont_diff.any(axis=1))\n",
    "        # new continous cf\n",
    "        c_cont_hat = torch.where(cont_diff, x_cont_inv, c_cont_inv)\n",
    "        c[:, :self.cat_idx] = self.scaler.transform(c_cont_hat)\n",
    "        c_y_hat = self.predict_fn(c)\n",
    "\n",
    "        self.diffs += (torch.round(c_y) != torch.round(c_y_hat)).sum()\n",
    "\n",
    "    def compute(self):\n",
    "        return 1 - self.diffs / self.total_n_changes, self.diffs, self.total_n_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((10, 4)) \n",
    "c = deepcopy(x)\n",
    "c[:5, :] = c[:5, :] + torch.rand((5, 4))\n",
    "c[5:, :] = c[5:, :] + torch.tensor([1.1, -2.1, 1.01, -1.2])\n",
    "\n",
    "pred_func = lambda arr: torch.mean(arr, dim=1) * 10\n",
    "\n",
    "scaler = StandardScaler().fit(x)\n",
    "c_y = pred_func(scaler.transform(c))\n",
    "\n",
    "\n",
    "sensitivity = SensitivityMetric(predict_fn=pred_func, scaler=scaler, cat_idx=4, threshold=1.)\n",
    "sensitivity.update(scaler.transform(x), scaler.transform(c), c_y)\n",
    "score, diffs, total_n_changes = sensitivity.compute()\n",
    "\n",
    "assert torch.equal(score, torch.tensor(0.))\n",
    "assert torch.equal(diffs, torch.tensor(5))\n",
    "assert torch.equal(total_n_changes, torch.tensor(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((10, 4)) \n",
    "c = x + torch.tensor([1.1, 0, 0, -1.1])\n",
    "c[:, 1:3] = c[:, 1:3] + torch.rand((10, 2))\n",
    "\n",
    "pred_func = lambda x: torch.mean(x, dim=1) * 10\n",
    "c_y = pred_func(scaler.transform(c))\n",
    "scaler = StandardScaler().fit(x)\n",
    "\n",
    "sensitivity = SensitivityMetric(predict_fn=pred_func, scaler=scaler, cat_idx=4, threshold=1.)\n",
    "sensitivity.update(scaler.transform(x), scaler.transform(c), c_y)\n",
    "score, diffs, total_n_changes = sensitivity.compute()\n",
    "\n",
    "assert torch.equal(score, torch.tensor(0.))\n",
    "assert torch.equal(diffs, torch.tensor(10))\n",
    "assert torch.equal(total_n_changes, torch.tensor(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export utils.functional\n",
    "def l1_mean(x, c):\n",
    "    return F.l1_loss(x, c, reduction='mean') / x.abs().mean() # MAD\n",
    "\n",
    "def get_loss_functions(f_name: str):\n",
    "    _loss_functions = {\n",
    "        'cross_entropy': F.binary_cross_entropy,\n",
    "        'l1': F.l1_loss,\n",
    "        'l1_mean': l1_mean,\n",
    "        'mse': F.mse_loss\n",
    "    }\n",
    "\n",
    "    assert f_name in _loss_functions.keys(), \\\n",
    "        f'function name `{f_name}` is not in the loss function list {_loss_functions.keys()}'\n",
    "\n",
    "    return _loss_functions[f_name]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export utils.functional\n",
    "def split_X_y(data: pd.DataFrame):\n",
    "    X = data[data.columns[:-1]]\n",
    "    y = data[data.columns[-1]]\n",
    "    return X, y\n",
    "\n",
    "def train_val_test_split(X, y):\n",
    "    assert len(X) == len(y)\n",
    "    size = len(X)\n",
    "    train_size = int(0.7 * size)    # 70% for training\n",
    "    val_size = int(0.8 * size)      # 10% for validation\n",
    "\n",
    "    return tuple(\n",
    "        tuple(X[: train_size], y[: train_size]),\n",
    "        tuple(X[train_size:val_size], y[train_size:val_size]),\n",
    "        tuple(X[val_size:], y[val_size:])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ABCBaseModule(ABC):\n",
    "    @abstractmethod\n",
    "    def model_forward(self, *x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, *x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, *x):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class BaseModule(pl.LightningModule, ABCBaseModule):\n",
    "    def __init__(self, configs: Dict[str, Any]):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(configs)\n",
    "\n",
    "        # read data\n",
    "        self.data = pd.read_csv(Path(configs['data_dir']))\n",
    "        self.continous_cols = configs['continous_cols']\n",
    "        self.discret_cols = configs['discret_cols']\n",
    "        self.__check_cols()\n",
    "\n",
    "        # set configss\n",
    "        self.lr = configs['lr']\n",
    "        self.batch_size = configs['batch_size']\n",
    "        self.lambda_1 = configs['lambda_1'] if 'lambda_1' in configs.keys() else 1\n",
    "        self.lambda_2 = configs['lambda_2'] if 'lambda_2' in configs.keys() else 1\n",
    "        self.lambda_3 = configs['lambda_3'] if 'lambda_3' in configs.keys() else 1\n",
    "        self.threshold = configs['threshold'] if 'threshold' in configs.keys() else 1\n",
    "        self.smooth_y = configs['smooth_y'] if 'smooth_y' in configs.keys() else True\n",
    "\n",
    "        # loss functions\n",
    "        self.loss_func_1 = get_loss_functions(configs['loss_1']) if 'loss_1' in configs.keys() else get_loss_functions(\"mse\")\n",
    "        self.loss_func_2 = get_loss_functions(configs['loss_2']) if 'loss_2' in configs.keys() else get_loss_functions(\"mse\")\n",
    "        self.loss_func_3 = get_loss_functions(configs['loss_3']) if 'loss_3' in configs.keys() else get_loss_functions(\"mse\")\n",
    "\n",
    "        # set model configss\n",
    "        self.enc_dims = configs['encoder_dims'] if 'encoder_dims' in configs.keys() else []\n",
    "        self.dec_dims = configs['decoder_dims'] if 'decoder_dims' in configs.keys() else []\n",
    "        self.exp_dims = configs['explainer_dims'] if 'explainer_dims' in configs.keys() else []\n",
    "\n",
    "        # metrics\n",
    "        self.accuracy = Accuracy()\n",
    "\n",
    "        # log graph\n",
    "        self.example_input_array = torch.randn((1, self.enc_dims[0]))\n",
    "\n",
    "    def __check_cols(self):\n",
    "        assert sorted(list(self.data.columns)) == sorted(self.continous_cols + self.discret_cols)\n",
    "        self.data = self.data.astype({col: np.float for col in self.continous_cols})\n",
    "\n",
    "    def training_epoch_end(self, outs):\n",
    "        if self.current_epoch == 0:\n",
    "            self.logger.log_hyperparams(self.hparams)\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # TODO Decouple data preparision and use `LightningDataModule`\n",
    "        # 70% for training, 10% for validation, 20% for testing\n",
    "        X, y = split_X_y(self.data)\n",
    "\n",
    "        # preprocessing\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.ohe = OneHotEncoder()\n",
    "        X_cont = self.scaler.fit_transform(X[self.continous_cols]) if self.continous_cols else np.array([[] for _ in range(len(X))])\n",
    "        X_cat = self.ohe.fit_transform(X[self.discret_cols]) if self.discret_cols else np.array([[] for _ in range(len(X))])\n",
    "        X = torch.cat((X_cont, X_cat), dim=1)\n",
    "\n",
    "        # init categorical normalizer to enable categorical features to be one-hot-encoding format\n",
    "        cat_arrays = self.ohe.categories_ if self.discret_cols else []\n",
    "        self.cat_normalizer = CategoricalNormalizer(cat_arrays, cat_idx=len(X_cont))\n",
    "\n",
    "        # init sensitivity metric\n",
    "        self.sensitivity = SensitivityMetric(\n",
    "            predict_fn=self.predict, scaler=self.scaler, cat_idx=len(X_cont), threshold=self.threshold)\n",
    "\n",
    "        pl_logger.info(f\"x_cont: {X_cont.size()}, x_cat: {X_cat.size()}\")\n",
    "        pl_logger.info(\"X shape: \", X.size())\n",
    "\n",
    "        assert X.size(-1) == self.enc_dims[0],\\\n",
    "            f'The input dimension X (shape: {X.shape[-1]})  != encoder_dims[0]: {self.enc_dims}'\n",
    "\n",
    "        # prepare train & test\n",
    "        train, val, test = train_val_test_split(X, y.to_numpy())\n",
    "        self.train_dataset = NumpyDataset(*train)\n",
    "        self.val_dataset = NumpyDataset(*val)\n",
    "        self.test_dataset = NumpyDataset(*test)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size,\n",
    "                          pin_memory=True, shuffle=True, num_workers=0)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size,\n",
    "                          pin_memory=True, shuffle=False, num_workers=0)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size,\n",
    "                          pin_memory=True, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def uniform(shape: tuple, r1: float, r2: float, device=None):\n",
    "    assert r1 < r2, f\"Issue: r1 ({r1}) >= r2 ({r2})\"\n",
    "    return (r2 - r1) * torch.rand(*shape, device=device) + r1\n",
    "\n",
    "\n",
    "def smooth_y(y, device=None):\n",
    "    return torch.where(y == 1,\n",
    "        uniform(y.size(), 0.8, 0.95, device=y.device),\n",
    "        uniform(y.size(), 0.05, 0.2, device=y.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of counternet.utils.processing failed: Traceback (most recent call last):\n",
      "  File \"/home/birk/software/miniconda3/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/birk/software/miniconda3/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/home/birk/software/miniconda3/lib/python3.8/imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/home/birk/software/miniconda3/lib/python3.8/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 604, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 783, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"../counternet/utils/processing.py\", line 81, in <module>\n",
      "    class OneHotEncoder(object):\n",
      "  File \"../counternet/utils/processing.py\", line 82, in OneHotEncoder\n",
      "    self.categories_ = []\n",
      "NameError: name 'self' is not defined\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# export\n",
    "class PredictionTrainingModule(BaseModule):\n",
    "    def forward(self, *x):\n",
    "        return self.model_forward(x)\n",
    "\n",
    "    def predict(self, x):\n",
    "        self.freeze()\n",
    "        y_hat = self(x)\n",
    "        return torch.round(y_hat)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam([p for p in self.parameters() if p.requires_grad], lr=self.lr)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # batch\n",
    "        *x, y = batch\n",
    "        # fwd\n",
    "        y_hat = self(*x)\n",
    "        # loss\n",
    "        if self.smooth_y:\n",
    "            y = smooth_y(y)\n",
    "        loss = F.binary_cross_entropy(y_hat, y)\n",
    "        score = self.accuracy(y_hat, y)\n",
    "        # Logging to TensorBoard\n",
    "        self.log('train/train_loss_1', loss, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        self.log('train/pred_accuracy', score, on_step=False, on_epoch=True, prog_bar=False, logger=True, sync_dist=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # batch\n",
    "        *x, y = batch\n",
    "        # fwd\n",
    "        y_hat = self(*x)\n",
    "        # loss\n",
    "        loss = F.binary_cross_entropy(y_hat, y)\n",
    "        score = self.accuracy(torch.round(y_hat), torch.round(y))\n",
    "        self.log('val/val_loss_1', loss, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        self.log('val/pred_accuracy', score, on_step=False, on_epoch=True, prog_bar=False, logger=True, sync_dist=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearBlock(nn.Module):\n",
    "    def __init__(self, input_dim, out_dim, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(input_dim, out_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class MultilayerPerception(nn.Module):\n",
    "    def __init__(self, dims=[3, 100, 10]):\n",
    "        super().__init__()\n",
    "        layers  = []\n",
    "        num_blocks = len(dims)\n",
    "        for i in range(1, num_blocks):\n",
    "            layers += [\n",
    "                LinearBlock(dims[i-1], dims[i])\n",
    "            ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class BaselinePredictiveModel(PredictionTrainingModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        assert self.enc_dims[-1] == self.dec_dims[0]\n",
    "        self.model = nn.Sequential(\n",
    "            MultilayerPerception(self.enc_dims),\n",
    "            MultilayerPerception(self.dec_dims),\n",
    "            nn.Linear(self.dec_dims[-1], 1)\n",
    "        )\n",
    "\n",
    "    def model_forward(self, x):\n",
    "        # x = ([],)\n",
    "        x, = x\n",
    "        y_hat = torch.sigmoid(self.model(x))\n",
    "        return torch.squeeze(y_hat, -1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
