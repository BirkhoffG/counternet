{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp utils.processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from ipynb_path import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 31\n"
     ]
    }
   ],
   "source": [
    "# export\n",
    "from counternet.import_essentials import *\n",
    "from counternet.utils.functional import *\n",
    "from counternet.utils.dataset import load_adult_income_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 742 ms, sys: 90.2 ms, total: 832 ms\nWall time: 826 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dummy_data = pd.read_csv('assets/data/dummy_data.csv')\n",
    "adult_data = load_adult_income_dataset('assets/data/adult.data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprossing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "class ABCScaler(ABC):\n",
    "    @abstractmethod\n",
    "    def fit(self, X):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def transform(self, X):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit_transform(self, X):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def inverse_transform(self, X):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class StandardScaler(ABCScaler):\n",
    "    \"\"\"rewrite `StandardScaler` object in sci-kit learn in pytorch to eliminate cpu-gpu communication time\"\"\"\n",
    "    mean_, std_ = None, None\n",
    "\n",
    "    @check_object_input_type\n",
    "    def fit(self, X):\n",
    "        self.mean_, self.std_ = torch.mean(X), torch.std(X)\n",
    "        return self\n",
    "\n",
    "    @check_object_input_type\n",
    "    def transform(self, X):\n",
    "        if (self.mean_ is None) or (self.std_ is None):\n",
    "            raise NotImplementedError(f'The scaler has not been fitted.')\n",
    "        return (X - self.mean_) / self.std_\n",
    "\n",
    "    @check_object_input_type\n",
    "    def fit_transform(self, X):\n",
    "        self.mean_, self.std_ = torch.mean(X), torch.std(X)\n",
    "        return (X - self.mean_) / self.std_\n",
    "\n",
    "    @check_object_input_type\n",
    "    def inverse_transform(self, X):\n",
    "        return X * self.std_ + self.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class MinMaxScaler(ABCScaler):\n",
    "    \"\"\"rewrite `MinMaxScaler` object in sci-kit learn in pytorch to eliminate cpu-gpu communication time\"\"\"\n",
    "    min_, max_ = None, None\n",
    "\n",
    "    @check_object_input_type\n",
    "    def fit(self, X):\n",
    "        self.min_, self.max_ = torch.min(X), torch.max(X)\n",
    "        assert self.min_ != self.max_, f\"min(X) == max(X) is not allowed.\"\n",
    "        return self\n",
    "\n",
    "    @check_object_input_type\n",
    "    def transform(self, X):\n",
    "        if (self.min_ is None) or (self.max_ is None):\n",
    "            raise NotImplementedError(f'The scaler has not been fitted.')\n",
    "        return (X - self.min_) / (self.max_ - self.min_)\n",
    "\n",
    "    @check_object_input_type\n",
    "    def fit_transform(self, X):\n",
    "        self.min_, self.max_ = torch.min(X), torch.max(X)\n",
    "        assert self.min_ != self.max_, f\"min(X) == max(X) is not allowed.\"\n",
    "        return (X - self.min_) / (self.max_ - self.min_)\n",
    "\n",
    "    @check_object_input_type\n",
    "    def inverse_transform(self, X):\n",
    "        return X * (self.max_ - self.min_) + self.min_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# TODO need to check\n",
    "class OneHotEncoder(object):\n",
    "    categories_ = []\n",
    "    drop_idx_ = None\n",
    "\n",
    "    def __init__(self):\n",
    "        from sklearn.preprocessing import OneHotEncoder\n",
    "        self.enc = OneHotEncoder(sparse=False)\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.enc.fit(X)\n",
    "        # copy attributes\n",
    "        self.categories_ = self.enc.categories_\n",
    "        self.drop_idx_ = self.enc.drop_idx_\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return torch.from_numpy(self.enc.transform(X))\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        assert isinstance(X, torch.Tensor)\n",
    "        return self.enc.inverse_transform(X.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar = StandardScaler()\n",
    "_X = adult_data[['age', 'hours_per_week']].to_numpy()\n",
    "cont = scalar.fit_transform(_X)\n",
    "assert not False in torch.isclose(torch.from_numpy(_X).float(), scalar.inverse_transform(cont))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar = MinMaxScaler()\n",
    "_X = adult_data[['age', 'hours_per_week']].to_numpy()\n",
    "cont = scalar.fit_transform(_X)\n",
    "assert (torch.isclose(torch.from_numpy(_X).float(), scalar.inverse_transform(cont))).all()\n",
    "assert ((0 <= cont) & (cont <= 1)).all()\n",
    "\n",
    "scalar = MinMaxScaler()\n",
    "_X = adult_data[['age', 'hours_per_week']].to_numpy()\n",
    "scalar.fit(_X)\n",
    "cont = scalar.transform(_X)\n",
    "assert (torch.isclose(torch.from_numpy(_X).float(), scalar.inverse_transform(cont))).all()\n",
    "assert ((0 <= cont) & (cont <= 1)).all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder()\n",
    "_X = adult_data[['workclass','education', 'marital_status', \n",
    "            'occupation','race', 'gender']]\n",
    "cat  = enc.fit_transform(_X)\n",
    "assert np.array_equal(enc.inverse_transform(cat), _X.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class NumpyDataset(TensorDataset):\n",
    "    def __init__(self, *arrs):\n",
    "        super().__init__()\n",
    "        # init tensors\n",
    "        # small patch: skip continous or discrete array without content\n",
    "        self.tensors = [torch.tensor(arr).float()\n",
    "                        for arr in arrs if arr.shape[-1] != 0]\n",
    "        assert all(self.tensors[0].size(0) == tensor.size(0)\n",
    "                   for tensor in self.tensors)\n",
    "\n",
    "    def data_loader(self, batch_size=128, shuffle=True, num_workers=4):\n",
    "        return DataLoader(self, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
    "\n",
    "    def features(self, test=False):\n",
    "        return tuple(self.tensors[:-1] if not test else self.tensors)\n",
    "\n",
    "    def target(self, test=False):\n",
    "        return self.tensors[-1] if not test else None\n",
    "\n",
    "\n",
    "class PandasDataset(NumpyDataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        cols = df.columns\n",
    "        X = df[cols[:-1]].to_numpy()\n",
    "        y = df[cols[-1]].to_numpy()\n",
    "        super().__init__(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.normal(50, 15, 100)\n",
    "y = np.random.normal(50, 15, 100)\n",
    "df_test = pd.DataFrame({'x': x, 'y': y})\n",
    "arrs = np.column_stack((x, y))\n",
    "np_dataset = NumpyDataset(x, y)\n",
    "pd_dataset = PandasDataset(df_test)\n",
    "\n",
    "assert (arrs == df_test.to_numpy()).all()\n",
    "assert len(np_dataset) == len(pd_dataset)\n",
    "assert (np.column_stack((x, y)) == df_test.to_numpy()).all()\n",
    "\n",
    "for i in range(len(np_dataset)):\n",
    "    assert np_dataset[i] == pd_dataset[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
