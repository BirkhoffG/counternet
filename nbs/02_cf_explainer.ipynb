{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp cf_explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 31\n"
     ]
    }
   ],
   "source": [
    "# export\n",
    "from counternet.import_essentials import *\n",
    "from counternet.utils import CategoricalNormalizer, flip_binary, load_configs, l1_mean, hinge_loss\n",
    "from counternet.base_interface import ExplainerBase, LocalExplainerBase, GlobalExplainerBase\n",
    "from counternet.training_module import BaseModule\n",
    "from counternet.model import MultilayerPerception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Clamp(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Clamp parameter to [0, 1]\n",
    "    code from: https://discuss.pytorch.org/t/regarding-clamped-learnable-parameter/58474/4\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        return input.clamp(min=0, max=1)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class VanillaCF(LocalExplainerBase):\n",
    "    def __init__(self, pred_fn: Callable[[torch.Tensor], torch.Tensor],\n",
    "                       cat_normalizer: CategoricalNormalizer=None, configs: Dict[str, Any]={}):\n",
    "        \"\"\"vanilla version of counterfactual generation\n",
    "            - link: https://doi.org/10.2139/ssrn.3063289\n",
    "        Args:\n",
    "        \"\"\"\n",
    "        super().__init__(pred_fn, configs)\n",
    "        self.steps = configs.steps if 'steps' in configs else 1000\n",
    "        self.cat_normalizer = cat_normalizer\n",
    "        self.cf = None\n",
    "\n",
    "    def forward(self):\n",
    "        if self.cf is None:\n",
    "            raise NotImplementedError('cf has not been initialized...')\n",
    "        cf = self.cf * 1.0\n",
    "        return cf if self.cat_normalizer is None else self.cat_normalizer.normalize(cf)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.RMSprop([self.cf], lr=0.001)\n",
    "\n",
    "    def _loss_functions(self, x, c):\n",
    "        # target\n",
    "        y_pred = self.pred_fn(x)\n",
    "        y_prime = flip_binary(y_pred)\n",
    "\n",
    "        c_y = self.pred_fn(c)\n",
    "        l_1 = F.binary_cross_entropy(c_y, y_prime)\n",
    "        l_2 = F.mse_loss(x, c)\n",
    "        return l_1, l_2\n",
    "\n",
    "    def _loss_compute(self, l_1, l_2):\n",
    "        return 1.0 * l_1 + 0.5 * l_2\n",
    "\n",
    "    def generate_cf(self, x: torch.Tensor):\n",
    "        self.cf = nn.Parameter(x.clone(), requires_grad=True)\n",
    "        optim = self.configure_optimizers()\n",
    "        for i in range(self.steps):\n",
    "            c = self()\n",
    "            l_1, l_2 = self._loss_functions(x, c)\n",
    "            loss = self._loss_compute(l_1, l_2)\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "        cf = self.cf.clone().detach() * 1.0\n",
    "        return cf if self.cat_normalizer is None else self.cat_normalizer.normalize(cf, hard=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DiverseCF(LocalExplainerBase):\n",
    "    def __init__(self, pred_fn: Callable[[torch.Tensor], torch.Tensor],\n",
    "                       cat_normalizer: CategoricalNormalizer=None, configs: Dict[str, Any]={}):\n",
    "        \"\"\"diverse counterfactual explanation\n",
    "            - link: https://doi.org/10.1145/3351095.3372850\n",
    "        Args:\n",
    "        \"\"\"\n",
    "        super().__init__(pred_fn, configs)\n",
    "        self.steps = configs.steps if 'steps' in configs else 1000\n",
    "        self.cat_normalizer = cat_normalizer\n",
    "        self.n_cfs = configs.n_cfs if 'n_cfs' in configs else 5\n",
    "        self.cf = None\n",
    "\n",
    "    def forward(self):\n",
    "        cf = self.cf * 1.0\n",
    "        return cf\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.RMSprop([self.cf], lr=0.001)\n",
    "\n",
    "    def _compute_dist(self, x1, x2):\n",
    "        return torch.sum(torch.abs(x1 - x2), dim = 0)\n",
    "\n",
    "    def _compute_proximity_loss(self):\n",
    "        \"\"\"Compute the second part (distance from x1) of the loss function.\"\"\"\n",
    "        proximity_loss = 0.0\n",
    "        for i in range(self.n_cfs):\n",
    "            proximity_loss += self.compute_dist(self.cf[i], self.x1)\n",
    "        return proximity_loss/(torch.mul(len(self.minx[0]), self.total_CFs))\n",
    "\n",
    "    def _dpp_style(self, cf):\n",
    "        det_entries = torch.ones(self.n_cfs, self.n_cfs)\n",
    "        for i in range(self.n_cfs):\n",
    "            for j in range(self.n_cfs):\n",
    "                det_entries[i, j] = self._compute_dist(cf[i], cf[j])\n",
    "\n",
    "        # implement inverse distance\n",
    "        det_entries = 1.0 / (1.0 + det_entries)\n",
    "        det_entries += torch.eye(self.n_cfs) * 0.0001\n",
    "        return torch.det(det_entries)\n",
    "\n",
    "    def _compute_diverse_loss(self, cf):\n",
    "        return self._dpp_style(cf)\n",
    "\n",
    "    def _compute_regularization_loss(self):\n",
    "        if self.cat_normalizer is None:\n",
    "            return 0.\n",
    "        \n",
    "        cat_idx = self.cat_normalizer.cat_idx\n",
    "        regularization_loss = 0.\n",
    "        for i in range(self.n_cfs):\n",
    "            for col in self.cat_normalizer.categories:\n",
    "                cat_idx_end = cat_idx + len(col)\n",
    "                regularization_loss += torch.pow((torch.sum(self.cf[i][cat_idx: cat_idx_end]) - 1.0), 2)\n",
    "        return regularization_loss\n",
    "\n",
    "    def _loss_functions(self, x, c):\n",
    "        # target\n",
    "        y_pred = self.pred_fn(x)\n",
    "        y_prime = torch.ones(y_pred.shape) - y_pred\n",
    "\n",
    "        c_y = self.pred_fn(c)\n",
    "        # yloss\n",
    "        l_1 = hinge_loss(input=c_y, target=y_prime.float())\n",
    "        # proximity loss\n",
    "        l_2 = l1_mean(x, c)\n",
    "        # diverse loss\n",
    "        l_3 = self._compute_diverse_loss(c)\n",
    "        # categorical penalty\n",
    "        l_4 = self._compute_regularization_loss()\n",
    "        return l_1, l_2, l_3, l_4\n",
    "\n",
    "    def _compute_loss(self, *loss_f):\n",
    "        return sum(loss_f)\n",
    "\n",
    "    def generate_cf(self, x: torch.Tensor):\n",
    "        self.cf = nn.Parameter(torch.rand(self.n_cfs, x.size(1)), requires_grad=True) \n",
    "        optim = self.configure_optimizers()\n",
    "        for i in range(self.steps):\n",
    "            c = self()\n",
    "\n",
    "            l_1, l_2, l_3, l_4 = self._loss_functions(x, c)\n",
    "            loss = self._compute_loss(l_1, l_2, l_3, l_4)\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "        cf = self.cf.clone().detach() * 1.0\n",
    "        cf = torch.clamp(cf, 0, 1)\n",
    "        # return cf[0]\n",
    "        return cf if self.cat_normalizer is None else self.cat_normalizer.normalize(cf, hard=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporti\n",
    "class AE(BaseModule):\n",
    "    def __init__(self, configs, encoded_size=5):\n",
    "        super().__init__(configs)\n",
    "        input_dim = configs['encoder_dims'][0]\n",
    "        self.encoder_model = MultilayerPerception([input_dim, 20, 16, 14, 12, encoded_size])\n",
    "        self.decoder_model = MultilayerPerception([encoded_size, 12, 14, 16, 20, input_dim])\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoded(x)\n",
    "        x_prime = self.decoder_model(z)\n",
    "        return x_prime\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam([p for p in self.parameters() if p.requires_grad], lr=self.lr)\n",
    "\n",
    "    def encoded(self, x):\n",
    "        return self.encoder_model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # batch\n",
    "        x, _ = batch\n",
    "        # prediction\n",
    "        x_prime = self(x)\n",
    "\n",
    "        loss = F.mse_loss(x_prime, x, reduction='mean')\n",
    "\n",
    "        self.log('train/loss', loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # batch\n",
    "        x, _ = batch\n",
    "        # prediction\n",
    "        x_prime = self(x)\n",
    "\n",
    "        loss = F.mse_loss(x_prime, x, reduction='mean')\n",
    "\n",
    "        self.log('val/val_loss', loss)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporti\n",
    "class VAE(pl.LightningModule):\n",
    "    def __init__(self, input_dims, encoded_size=5):\n",
    "        super().__init__()\n",
    "        self.encoder_mean = MultilayerPerception([input_dims + 1, 20, 16, 14, 12, encoded_size])\n",
    "        self.encoder_var = MultilayerPerception([input_dims + 1, 20, 16, 14, 12, encoded_size])\n",
    "        self.decoder_mean = MultilayerPerception([encoded_size + 1, 12, 14, 16, 20, input_dims])\n",
    "\n",
    "    def encoder(self, x):\n",
    "        mean = self.encoder_mean(x)\n",
    "        logvar = 0.5+ self.encoder_var(x)\n",
    "        return mean, logvar\n",
    "\n",
    "    def decoder(self, z):\n",
    "        mean = self.decoder_mean(z)\n",
    "        return mean\n",
    "\n",
    "    def sample_latent_code(self, mean, logvar):\n",
    "        eps = torch.randn_like(logvar)\n",
    "        return mean + torch.sqrt(logvar) * eps\n",
    "\n",
    "    def normal_likelihood(self, x, mean, logvar, raxis=1):\n",
    "        return torch.sum( -.5 * ((x - mean)*(1./logvar)*(x-mean) + torch.log(logvar) ), axis=1)\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        \"\"\"\n",
    "        x: input instance\n",
    "        c: target y\n",
    "        \"\"\"\n",
    "        c = c.view(c.shape[0], 1)\n",
    "        c = c.clone().detach().float\n",
    "        res = {}\n",
    "        mc_samples = 50\n",
    "        em, ev = self.encoder(torch.cat((x, c), 1))\n",
    "        res['em'] = em\n",
    "        res['ev'] = ev\n",
    "        res['z'] = []\n",
    "        res['x_pred'] = []\n",
    "        res['mc_samples'] = mc_samples\n",
    "        for i in range(mc_samples):\n",
    "            z = self.sample_latent_code(em, ev)\n",
    "            x_pred = self.decoder(torch.cat((z, c), 1))\n",
    "            res['z'].append(z)\n",
    "            res['x_pred'].append(x_pred)\n",
    "        return res\n",
    "\n",
    "    def compute_elbo(self, x, c, model):\n",
    "        c= c.clone().detach().float()\n",
    "        c=c.view(c.shape[0], 1)\n",
    "        em, ev = self.encoder(torch.cat((x,c),1))\n",
    "        kl_divergence = 0.5*torch.mean(em**2 + ev - torch.log(ev) - 1, axis=1)\n",
    "\n",
    "        z = self.sample_latent_code(em, ev)\n",
    "        dm= self.decoder( torch.cat((z,c),1) )\n",
    "        log_px_z = torch.tensor(0.0)\n",
    "\n",
    "        x_pred= dm\n",
    "        return torch.mean(log_px_z), torch.mean(kl_divergence), x, x_pred, model.predict(x_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class VAE_CF(BaseModule, GlobalExplainerBase):\n",
    "    def __init__(self, config: Dict, pred_model: pl.LightningModule):\n",
    "        \"\"\"\n",
    "        config: basic configs\n",
    "        model: the black-box model to be explained\n",
    "        \"\"\"\n",
    "        super().__init__(config)\n",
    "        self.pred_model = pred_model\n",
    "        self.pred_model.freeze()\n",
    "        self.vae = VAE(input_dims=self.enc_dims[0])\n",
    "        # validity_reg set to 42.0\n",
    "        # according to https://interpret.ml/DiCE/notebooks/DiCE_getting_started_feasible.html#Generate-counterfactuals-using-a-VAE-model\n",
    "        self.validity_reg = config['validity_reg'] if 'validity_reg' in config.keys() else 1.0\n",
    "\n",
    "    def model_forward(self, x):\n",
    "        \"\"\"lazy implementation since this method will not be used\"\"\"\n",
    "        recon_err, kl_err, x_true, x_pred, cf_label = self.vae.compute_elbo(x, 1 - self.pred_model.predict(x), self.pred_model)\n",
    "        # return y, c\n",
    "        return cf_label, x_pred\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"lazy implementation since this method will not be used\"\"\"\n",
    "        return self.model_forward(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam([p for p in self.parameters() if p.requires_grad], lr=self.lr)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.pred_model.predict(x)\n",
    "\n",
    "    def compute_loss(self, out, x, y):\n",
    "        em = out['em']\n",
    "        ev = out['ev']\n",
    "        z = out['z']\n",
    "        dm = out['x_pred']\n",
    "        mc_samples = out['mc_samples']\n",
    "        #KL Divergence\n",
    "        kl_divergence = 0.5*torch.mean(em**2 + ev - torch.log(ev) - 1, axis=1)\n",
    "\n",
    "        #Reconstruction Term\n",
    "        #Proximity: L1 Loss\n",
    "        x_pred = dm[0]\n",
    "        cat_idx = len(self.continous_cols)\n",
    "        # recon_err = - \\\n",
    "        #     torch.sum(torch.abs(x[:, cat_idx:-1] -\n",
    "        #                         x_pred[:, cat_idx:-1]), axis=1)\n",
    "        recon_err = - torch.sum(torch.abs(x - x_pred), axis=1)\n",
    "\n",
    "        # Sum to 1 over the categorical indexes of a feature\n",
    "        for col in self.cat_normalizer.categories:\n",
    "            cat_end_idx = cat_idx + len(col)\n",
    "            temp = - \\\n",
    "                torch.abs(1.0 - x_pred[:, cat_idx: cat_end_idx].sum(axis=1))\n",
    "            recon_err += temp\n",
    "\n",
    "        #Validity\n",
    "        c_y = self.pred_model(x_pred)\n",
    "        validity_loss = torch.zeros(1, device=self.device)\n",
    "        validity_loss += hinge_loss(input=c_y, target=y.float())\n",
    "\n",
    "        for i in range(1, mc_samples):\n",
    "            x_pred = dm[i]\n",
    "\n",
    "            # recon_err += - \\\n",
    "            #     torch.sum(torch.abs(x[:, cat_idx:-1] -\n",
    "            #                         x_pred[:, cat_idx:-1]), axis=1)\n",
    "            recon_err += - torch.sum(torch.abs(x - x_pred), axis=1)\n",
    "\n",
    "            # Sum to 1 over the categorical indexes of a feature\n",
    "            for col in self.cat_normalizer.categories:\n",
    "                cat_end_idx = cat_idx + len(col)\n",
    "                temp = - \\\n",
    "                    torch.abs(1.0 - x_pred[:, cat_idx: cat_end_idx].sum(axis=1))\n",
    "                recon_err += temp\n",
    "\n",
    "            #Validity\n",
    "            c_y = self.pred_model(x_pred)\n",
    "            validity_loss += hinge_loss(c_y, y.float())\n",
    "\n",
    "        recon_err = recon_err / mc_samples\n",
    "        validity_loss = -1 * self.validity_reg * validity_loss / mc_samples\n",
    "\n",
    "        return -torch.mean(recon_err - kl_divergence) - validity_loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # batch\n",
    "        x, _ = batch\n",
    "        # prediction\n",
    "        y_hat = self.pred_model.predict(x)\n",
    "        # target\n",
    "        y = 1.0 - y_hat\n",
    "\n",
    "        out = self.vae(x, y)\n",
    "        loss = self.compute_loss(out, x, y)\n",
    "\n",
    "        self.log('train/loss', loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # batch\n",
    "        x, _ = batch\n",
    "        # prediction\n",
    "        y_hat = self.pred_model.predict(x)\n",
    "        # target\n",
    "        y = 1.0 - y_hat\n",
    "\n",
    "        out = self.vae(x, y)\n",
    "        loss = self.compute_loss(out, x, y)\n",
    "\n",
    "        _, _, _, x_pred, cf_label = self.vae.compute_elbo(x, y, self.pred_model)\n",
    "\n",
    "        cf_proximity = torch.abs(x - x_pred).sum(dim=1).mean()\n",
    "        cf_accuracy = accuracy(cf_label, y.int())\n",
    "\n",
    "        self.log('val/val_loss', loss)\n",
    "        self.log('val/proximity', cf_proximity)\n",
    "        self.log('val/cf_accuracy', cf_accuracy)\n",
    "\n",
    "    def generate_cf(self, x):\n",
    "        self.vae.freeze()\n",
    "        y_hat = self.pred_model.predict(x)\n",
    "        recon_err, kl_err, x_true, x_pred, cf_label = self.vae.compute_elbo(x, 1.-y_hat, self.pred_model)\n",
    "        return self.pred_model.cat_normalizer.normalize(x_pred, hard=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
